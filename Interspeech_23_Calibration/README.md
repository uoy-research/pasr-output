# Automatic speaker recognition with variation across vocal conditions

This markdown file details the workflow for the Interspeech submission *Automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics* (include link if/when becomes available online)

## Test Data

| Participant Number | Participant initials |
| --- | --- |
| P0001 | PF |
| P0002 | JT |
| P0003 | BGR |
| P0004 | FN |
| P0006 | RO |
| P0009 | DW |

| VC Group | Vocal Conditions |
| --- | --- |
| Baseline | MOD - Modal voice |
| Accent Guises | RPR - Received Pronunciation
 ACC - Non-standard guises (*Geordie, Manchester, NYC, Yorkshire*)|
| Laryngeal | BRT - Breathy; CRK - Creaky; WHS - Whisper |
| Supralaryngeal | FTB - Fronted Tongue Body; BTB - Backed Tongue Body; RET - Retroflex; LLX - Lowered Larynx |
| Miscellaneous | HIG - High pitch; LOW - Low pitch; FAS - Fast; LIV - Lively; MON - Monotone; PEN - Pen between the teeth; PIN - Pinched nose |

The following conditions were not included in this paper:



(Explain why some excluded)


## Calibration Data
Include speaker codes for DyViS speakers


## Comps and scores

Include x-vectors (?)
Include links to many-to-many comparison data (save final .csv from R)
Link to version of R code used to wrangle from VOCALISE


### Analysis 

Include link to version of Matlab code used to do Score-to-LR conversion
Include link to version of Matlab code used to evaluate performance

## Results

Include plot from article on git and include link here. Also link to version of R code used to generate final plot
Include code or plots which weren't included in final article (?)